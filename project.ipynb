{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobertCall/FakeNewsNet/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "!pip install ufal.udpipe\n",
        "!pip install corpy\n",
        "!pip install -U pymorphy2-dicts-ru"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpvbZGoCrUgO",
        "outputId": "48d26836-fb63-477b-b8b5-6b0336a9c400"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 10.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=ed3bcbd120696b8d5c51ed621b56715227f591d5b3c32fc43cc533d0a372475c\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ufal.udpipe\n",
            "  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 5.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp38-cp38-linux_x86_64.whl size=5626903 sha256=887ecc587c8004705a9c864eb9c10fa0ef3781ccf4b87608beb6075140782546\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/c1/67/142cea91540458ab9edac9c280a19b549a03217d7b441d32a6\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe\n",
            "Successfully installed ufal.udpipe-1.2.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting corpy\n",
            "  Downloading corpy-0.4.1-py3-none-any.whl (37 kB)\n",
            "Collecting ufal.morphodita>=1.10\n",
            "  Downloading ufal.morphodita-1.11.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[K     |████████████████████████████████| 425 kB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.6.1 in /usr/local/lib/python3.8/dist-packages (from corpy) (4.9.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from corpy) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from corpy) (2022.6.2)\n",
            "Requirement already satisfied: wordcloud>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from corpy) (1.8.2.2)\n",
            "Collecting lazy>=1.4\n",
            "  Downloading lazy-1.5-py2.py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from corpy) (1.21.6)\n",
            "Requirement already satisfied: ufal.udpipe>=1.2 in /usr/local/lib/python3.8/dist-packages (from corpy) (1.2.0.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from wordcloud>=1.8.1->corpy) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from wordcloud>=1.8.1->corpy) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->wordcloud>=1.8.1->corpy) (1.15.0)\n",
            "Installing collected packages: ufal.morphodita, lazy, corpy\n",
            "Successfully installed corpy-0.4.1 lazy-1.5 ufal.morphodita-1.11.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymorphy2-dicts-ru in /usr/local/lib/python3.8/dist-packages (2.4.417127.4579844)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIIANnB7_B9X",
        "outputId": "799b7f75-d404-4069-a453-29362aa73d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import pymorphy2\n",
        "\n",
        "import os\n",
        "import gensim\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import ufal.udpipe as udp\n",
        "import corpy.udpipe as crp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten\n",
        "\n"
      ],
      "metadata": {
        "id": "s-w-4_Y1eCSJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6oOCtpSmh93",
        "outputId": "282fc935-4be7-4682-8a0d-d4ab8eda94ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "nltk_tokenizer = RegexpTokenizer(r'[а-яёa-z]+')\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def text_preprocessing(text):\n",
        "  words = nltk_tokenizer.tokenize(text.lower())\n",
        "  lem_text = [morph.parse(w)[0].normal_form for w in words if w not in stop_words]\n",
        "\n",
        "  return lem_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isfile('220.zip'):\n",
        "  !wget http://vectors.nlpl.eu/repository/20/220.zip\n",
        "  !unzip 220.zip\n",
        "\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)"
      ],
      "metadata": {
        "id": "ZAs-qTCEmj1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca09fa7a-2822-4854-e264-e8401d950e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-26 10:34:55--  http://vectors.nlpl.eu/repository/20/220.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 638171816 (609M) [application/zip]\n",
            "Saving to: ‘220.zip’\n",
            "\n",
            "220.zip             100%[===================>] 608.61M  18.6MB/s    in 35s     \n",
            "\n",
            "2022-12-26 10:35:30 (17.5 MB/s) - ‘220.zip’ saved [638171816/638171816]\n",
            "\n",
            "Archive:  220.zip\n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df_news = pd.read_csv ('news0.csv')\n",
        "df_news = pd.read_csv ('train.tsv', sep='\\t')\n",
        "test = pd.read_csv ('test.tsv', sep='\\t')"
      ],
      "metadata": {
        "id": "eZ4GKQeL24Zq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачивание модели UDPipe, обученную на русском языке\n",
        "udp_model_url = r'https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3131/russian-syntagrus-ud-2.5-191206.udpipe'\n",
        "udp_model_filename = 'russian-syntagrus-ud-2.5-191206.udpipe'\n",
        "#if not os.path.isfile(udp_model_filename):\n",
        " # !wget.download(udp_model_url)\n",
        "\n",
        "# Загрузка модели в оболочку corpy\n",
        "# corpy_model = udp.Model.load(udp_model_filename)\n",
        "corpy_model = crp.Model(udp_model_filename)\n",
        "print('model', corpy_model)\n",
        "\n",
        "# Функция для тегирования слов\n",
        "def udp_tagging(lem_text):\n",
        "  sents = [list(corpy_model.process(w)) for w in lem_text]\n",
        "  tagged_words = [s[0].words[1].form + '_' + s[0].words[1].upostag for s in sents if s]\n",
        "\n",
        "  return tagged_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lfpsEYkmnfj",
        "outputId": "aef71798-dbd3-43dd-a925-c1c44d74248b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model <corpy.udpipe.Model object at 0x7f54d2de9e80>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.most_similar(udp_tagging(['учитель'])) #Тестируем w2v на слове \"учитель\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLiGOVFhmr_F",
        "outputId": "098c6a30-bcc5-4056-fa23-e3c0dbb8c638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('учительница_NOUN', 0.7843227386474609),\n",
              " ('ученик_NOUN', 0.7229872941970825),\n",
              " ('преподаватель_NOUN', 0.6898813843727112),\n",
              " ('воспитатель_NOUN', 0.667870283126831),\n",
              " ('учитель_PROPN', 0.609573245048523),\n",
              " ('репетитор_NOUN', 0.6008316278457642),\n",
              " ('учить_VERB', 0.5971025228500366),\n",
              " ('завуч_NOUN', 0.5949769020080566),\n",
              " ('урок_NOUN', 0.5922257304191589),\n",
              " ('преподавательница_NOUN', 0.5865318775177002)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "X_not_filtered_texts = df_news['title'].apply(lambda x: udp_tagging(text_preprocessing(x))).array\n",
        "with open('data1.pickle', 'wb') as f:\n",
        "    pickle.dump(X_not_filtered_texts, f)\n",
        "#with open ('data.pickle', 'rb') as f:\n",
        "   #X_not_filtered_texts = pickle.load(f)\n",
        "\n",
        "X_filtered_texts = []\n",
        "for text in X_not_filtered_texts:\n",
        "  words = []\n",
        "  for word in text:\n",
        "    if word in w2v.vocab:\n",
        "      words.append(word)\n",
        "  if len(words) == 0:\n",
        "    print (\"asd\")\n",
        "  X_filtered_texts.append(words)"
      ],
      "metadata": {
        "id": "DOzyNAoSmz5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = [list(map(lambda x: float(w2v.vocab[x].index), t)) for t in X_filtered_texts]\n",
        "X = pad_sequences(X, maxlen=100)\n",
        "X = np.asarray(X).astype('float32')\n",
        "\n",
        "Y = np.array(df_news['is_fake']).astype('float32').reshape((-1,1))\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y)"
      ],
      "metadata": {
        "id": "bf55rhNGm0qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model = Sequential()\n",
        "\n",
        "weights = w2v.vectors\n",
        "layer = Embedding(\n",
        "    input_dim=weights.shape[0],\n",
        "    output_dim=weights.shape[1],\n",
        "    weights=[weights],\n",
        "    input_length=100,\n",
        "    mask_zero=True,\n",
        "    trainable=False,\n",
        ")\n",
        "\n",
        "seq_model.add(layer)\n",
        "seq_model.add(Dense(50, activation='relu'))\n",
        "seq_model.add(Flatten())\n",
        "seq_model.add(Dropout(0.6))\n",
        "seq_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "seq_model.compile(loss='mean_squared_logarithmic_error',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "seq_model.fit(x_train, y_train, epochs=20,\n",
        "               validation_data=(x_test, y_test))\n",
        "\n",
        "seq_model.summary()\n",
        "# seq_model.save(\"model.keras\")"
      ],
      "metadata": {
        "id": "hfZ6b96Sm4F6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7eb90a8-8549-4156-c771-4afc626bf33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "135/135 [==============================] - 3s 19ms/step - loss: 0.1616 - accuracy: 0.6042 - val_loss: 0.0755 - val_accuracy: 0.7715\n",
            "Epoch 2/20\n",
            "135/135 [==============================] - 2s 15ms/step - loss: 0.0778 - accuracy: 0.7649 - val_loss: 0.0648 - val_accuracy: 0.8111\n",
            "Epoch 3/20\n",
            "135/135 [==============================] - 2s 15ms/step - loss: 0.0680 - accuracy: 0.7990 - val_loss: 0.0618 - val_accuracy: 0.8174\n",
            "Epoch 4/20\n",
            "135/135 [==============================] - 2s 16ms/step - loss: 0.0626 - accuracy: 0.8194 - val_loss: 0.0601 - val_accuracy: 0.8146\n",
            "Epoch 5/20\n",
            "135/135 [==============================] - 2s 15ms/step - loss: 0.0595 - accuracy: 0.8247 - val_loss: 0.0573 - val_accuracy: 0.8299\n",
            "Epoch 6/20\n",
            "135/135 [==============================] - 2s 15ms/step - loss: 0.0562 - accuracy: 0.8351 - val_loss: 0.0574 - val_accuracy: 0.8306\n",
            "Epoch 7/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0536 - accuracy: 0.8430 - val_loss: 0.0554 - val_accuracy: 0.8347\n",
            "Epoch 8/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0502 - accuracy: 0.8573 - val_loss: 0.0555 - val_accuracy: 0.8347\n",
            "Epoch 9/20\n",
            "135/135 [==============================] - 2s 17ms/step - loss: 0.0520 - accuracy: 0.8504 - val_loss: 0.0552 - val_accuracy: 0.8410\n",
            "Epoch 10/20\n",
            "135/135 [==============================] - 2s 16ms/step - loss: 0.0498 - accuracy: 0.8539 - val_loss: 0.0549 - val_accuracy: 0.8396\n",
            "Epoch 11/20\n",
            "135/135 [==============================] - 2s 17ms/step - loss: 0.0471 - accuracy: 0.8622 - val_loss: 0.0542 - val_accuracy: 0.8500\n",
            "Epoch 12/20\n",
            "135/135 [==============================] - 2s 17ms/step - loss: 0.0464 - accuracy: 0.8675 - val_loss: 0.0538 - val_accuracy: 0.8417\n",
            "Epoch 13/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0447 - accuracy: 0.8736 - val_loss: 0.0547 - val_accuracy: 0.8479\n",
            "Epoch 14/20\n",
            "135/135 [==============================] - 3s 21ms/step - loss: 0.0440 - accuracy: 0.8738 - val_loss: 0.0540 - val_accuracy: 0.8458\n",
            "Epoch 15/20\n",
            "135/135 [==============================] - 3s 22ms/step - loss: 0.0418 - accuracy: 0.8752 - val_loss: 0.0556 - val_accuracy: 0.8500\n",
            "Epoch 16/20\n",
            "135/135 [==============================] - 2s 16ms/step - loss: 0.0425 - accuracy: 0.8803 - val_loss: 0.0546 - val_accuracy: 0.8465\n",
            "Epoch 17/20\n",
            "135/135 [==============================] - 2s 16ms/step - loss: 0.0421 - accuracy: 0.8784 - val_loss: 0.0546 - val_accuracy: 0.8417\n",
            "Epoch 18/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0380 - accuracy: 0.8949 - val_loss: 0.0550 - val_accuracy: 0.8458\n",
            "Epoch 19/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0389 - accuracy: 0.8893 - val_loss: 0.0556 - val_accuracy: 0.8486\n",
            "Epoch 20/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0382 - accuracy: 0.8930 - val_loss: 0.0557 - val_accuracy: 0.8479\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_12 (Embedding)    (None, 100, 300)          74799900  \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 100, 50)           15050     \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 5000)              0         \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 5000)              0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 1)                 5001      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 74,819,951\n",
            "Trainable params: 20,051\n",
            "Non-trainable params: 74,799,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "seq_model = load_model(\"model.keras\")"
      ],
      "metadata": {
        "id": "To6zVReldX-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_texts = ['Новак спрогнозировал новый мировой энергокризис через 5-10 лет',\n",
        "              'Во всех школах России установят счетчик американского госдолга',\n",
        "              'РФ продолжает рассматривать европу как потенциальный рынок для сбыта газа',\n",
        "              'Число погибших от последствий зимнего шторма в США выросло до 28',\n",
        "              'В магазинах появились дешевые яйца без желтков']"
      ],
      "metadata": {
        "id": "8Qk14NhygvnE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_preproc = [udp_tagging(text_preprocessing(fake_text)) for fake_text in fake_texts]\n",
        "\n",
        "fake_words_tagged = []\n",
        "for text in fake_preproc:\n",
        "  words = []\n",
        "  for word in text:\n",
        "    if word in w2v.vocab:\n",
        "      words.append(word)\n",
        "  fake_words_tagged.append(words)\n",
        "\n",
        "fake_indexed = [list(map(lambda x: float(w2v.vocab[x].index), t)) for t in fake_words_tagged]\n",
        "fake_indexed = pad_sequences(fake_indexed, maxlen=100)\n",
        "fake_indexed = np.asarray(fake_indexed).astype('float32')\n",
        "\n",
        "print(seq_model.predict(fake_indexed))"
      ],
      "metadata": {
        "id": "nJY-KzWHm5Hz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf999f5-6818-4b03-ba06-def2d2d3c219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "[[0.37466243]\n",
            " [0.6659432 ]\n",
            " [0.11183485]\n",
            " [0.7457472 ]\n",
            " [0.4915185 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv ('test.tsv', sep='\\t')\n",
        "fake_preproc = test['title'].apply(lambda x: udp_tagging(text_preprocessing(x))).array\n",
        "fake_words_tagged = []\n",
        "for text in fake_preproc:\n",
        "  words = []\n",
        "  for word in text:\n",
        "    if word in w2v.vocab:\n",
        "      words.append(word)\n",
        "  fake_words_tagged.append(words)\n",
        "Y = np.array(test['is_fake']).astype('float32').reshape((-1,1))\n",
        "\n",
        "fake_indexed = [list(map(lambda x: float(w2v.vocab[x].index), t)) for t in fake_words_tagged]\n",
        "fake_indexed = pad_sequences(fake_indexed, maxlen=100)\n",
        "fake_indexed = np.asarray(fake_indexed).astype('float32')\n",
        "\n",
        "print(seq_model.evaluate(fake_indexed, Y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcxDpi0tHznu",
        "outputId": "65e87ba5-ac78-4317-ee43-af36213d7e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 7ms/step - loss: 0.2039 - accuracy: 0.5410\n",
            "[0.2038509100675583, 0.5410000085830688]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "fake_preproc = df_news['title'].apply(lambda x: \" \".join(text_preprocessing(x))).array\n",
        "Y = np.array(df_news['is_fake']).astype('float32').reshape((-1,1))\n",
        "x_train, x_test, y_train, y_test = train_test_split(fake_preproc, Y)\n",
        "tfidf = TfidfVectorizer()\n",
        "vec_train = tfidf.fit_transform(x_train)\n",
        "vec_val = tfidf.transform(x_test)\n",
        "\n",
        "pac = PassiveAggressiveClassifier(C = 0.01)\n",
        "\n",
        "pac.fit(vec_train, y_train)\n",
        "val_pred = pac.predict(vec_val)\n",
        "\n",
        "\n",
        "print(classification_report(y_test, val_pred))\n",
        "fake_preproc = test['title'].apply(lambda x: \" \".join(text_preprocessing(x))).array\n",
        "Y = np.array(test['is_fake']).astype('float32').reshape((-1,1))\n",
        "vec_val = tfidf.transform(fake_preproc)\n",
        "val_pred = pac.predict(vec_val)\n",
        "print(classification_report(Y, val_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpYVJX4ONsn1",
        "outputId": "33ae4d10-a3e6-4504-e924-47df7e1a84dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.81      0.83       735\n",
            "         1.0       0.81      0.85      0.83       705\n",
            "\n",
            "    accuracy                           0.83      1440\n",
            "   macro avg       0.83      0.83      0.83      1440\n",
            "weighted avg       0.83      0.83      0.83      1440\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.49      0.65      1000\n",
            "         1.0       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.49      1000\n",
            "   macro avg       0.50      0.24      0.33      1000\n",
            "weighted avg       1.00      0.49      0.65      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fake_preproc = [\" \".join(text_preprocessing(fake_text)) for fake_text in fake_texts]\n",
        "vec_val = tfidf.transform(fake_preproc)\n",
        "val_pred = pac.decision_function(vec_val)\n",
        "val_pred0 = pac.predict(vec_val)\n",
        "print(val_pred, val_pred0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hHCmLFDfLFb",
        "outputId": "a32fc158-6fcd-4516-cc75-c6880977b93b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.79722638  1.0119444  -1.03961405 -0.4323088   0.33548233] [0. 1. 0. 0. 1.]\n"
          ]
        }
      ]
    }
  ]
}