{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobertCall/FakeNewsNet/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Классификатор фейковых новостей"
      ],
      "metadata": {
        "id": "8Z1Yh3SAR3nZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Нейронная сеть основанная на word2vec сети"
      ],
      "metadata": {
        "id": "W3vvi405SbZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачивание необходимых пакетов"
      ],
      "metadata": {
        "id": "hWVpOSCuSntN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "!pip install ufal.udpipe\n",
        "!pip install corpy\n",
        "!pip install -U pymorphy2-dicts-ru"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpvbZGoCrUgO",
        "outputId": "2e79b019-268d-4d56-aa40-4ab31b633934"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 52.1 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=c30cfd364fa2e01f7407177b104cb0c4afdb8e86eac01aa2989e8f23ada45bc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ufal.udpipe\n",
            "  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 16.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp38-cp38-linux_x86_64.whl size=5626996 sha256=0a34d205bd8e47438f8cc67e37359cf176fe6ab4c0cb7550e80b2fe108b2e117\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/c1/67/142cea91540458ab9edac9c280a19b549a03217d7b441d32a6\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe\n",
            "Successfully installed ufal.udpipe-1.2.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting corpy\n",
            "  Downloading corpy-0.4.1-py3-none-any.whl (37 kB)\n",
            "Collecting lazy>=1.4\n",
            "  Downloading lazy-1.5-py2.py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.6.1 in /usr/local/lib/python3.8/dist-packages (from corpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from corpy) (1.21.6)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from corpy) (7.1.2)\n",
            "Collecting ufal.morphodita>=1.10\n",
            "  Downloading ufal.morphodita-1.11.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[K     |████████████████████████████████| 425 kB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wordcloud>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from corpy) (1.8.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from corpy) (2022.6.2)\n",
            "Requirement already satisfied: ufal.udpipe>=1.2 in /usr/local/lib/python3.8/dist-packages (from corpy) (1.2.0.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from wordcloud>=1.8.1->corpy) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from wordcloud>=1.8.1->corpy) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->wordcloud>=1.8.1->corpy) (1.15.0)\n",
            "Installing collected packages: ufal.morphodita, lazy, corpy\n",
            "Successfully installed corpy-0.4.1 lazy-1.5 ufal.morphodita-1.11.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymorphy2-dicts-ru in /usr/local/lib/python3.8/dist-packages (2.4.417127.4579844)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорт пакетов"
      ],
      "metadata": {
        "id": "6R5JXrsPSr8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import pymorphy2\n",
        "\n",
        "import os\n",
        "import gensim\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import ufal.udpipe as udp\n",
        "import corpy.udpipe as crp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten"
      ],
      "metadata": {
        "id": "s-w-4_Y1eCSJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание токенайзера и лемматайзера. И создание функции для обработки строк"
      ],
      "metadata": {
        "id": "W_iKrwFNSwFq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6oOCtpSmh93",
        "outputId": "5adfc650-61ee-437e-8fe3-853123d70736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "nltk_tokenizer = RegexpTokenizer(r'[а-яёa-z]+')\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def text_preprocessing(text):\n",
        "  words = nltk_tokenizer.tokenize(text.lower())\n",
        "  lem_text = [morph.parse(w)[0].normal_form for w in words if w not in stop_words]\n",
        "\n",
        "  return lem_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачивание и создание модели word2vec"
      ],
      "metadata": {
        "id": "Z41a047QS7dZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isfile('220.zip'):\n",
        "  !wget http://vectors.nlpl.eu/repository/20/220.zip\n",
        "  !unzip 220.zip\n",
        "\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)"
      ],
      "metadata": {
        "id": "ZAs-qTCEmj1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e97caed-610a-47b7-973c-ad1cacec48f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-26 16:26:45--  http://vectors.nlpl.eu/repository/20/220.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 638171816 (609M) [application/zip]\n",
            "Saving to: ‘220.zip’\n",
            "\n",
            "220.zip             100%[===================>] 608.61M  26.9MB/s    in 23s     \n",
            "\n",
            "2022-12-26 16:27:10 (26.0 MB/s) - ‘220.zip’ saved [638171816/638171816]\n",
            "\n",
            "Archive:  220.zip\n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтение данных"
      ],
      "metadata": {
        "id": "6llpTk-YS_wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_news = pd.read_csv ('train.tsv', sep='\\t')\n",
        "test = pd.read_csv ('test.tsv', sep='\\t')"
      ],
      "metadata": {
        "id": "eZ4GKQeL24Zq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачивание и создание модели для тегирования слов"
      ],
      "metadata": {
        "id": "FCfJ6SdRTCYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачивание модели UDPipe, обученную на русском языке\n",
        "udp_model_url = r'https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3131/russian-syntagrus-ud-2.5-191206.udpipe'\n",
        "udp_model_filename = 'russian-syntagrus-ud-2.5-191206.udpipe'\n",
        "#if not os.path.isfile(udp_model_filename):\n",
        " # !wget.download(udp_model_url)\n",
        "\n",
        "# Загрузка модели в оболочку corpy\n",
        "# corpy_model = udp.Model.load(udp_model_filename)\n",
        "corpy_model = crp.Model(udp_model_filename)\n",
        "print('model', corpy_model)\n",
        "\n",
        "# Функция для тегирования слов\n",
        "def udp_tagging(lem_text):\n",
        "  sents = [list(corpy_model.process(w)) for w in lem_text]\n",
        "  tagged_words = [s[0].words[1].form + '_' + s[0].words[1].upostag for s in sents if s]\n",
        "\n",
        "  return tagged_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lfpsEYkmnfj",
        "outputId": "024e0ceb-266e-4318-f7ff-b122ec4ea9dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model <corpy.udpipe.Model object at 0x7fdc918ef760>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предобработка текста (лемматизация, теггирование и фильтрация)"
      ],
      "metadata": {
        "id": "7CGU1rzmTNzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "X_not_filtered_texts = df_news['title'].apply(lambda x: udp_tagging(text_preprocessing(x))).array\n",
        "with open('data1.pickle', 'wb') as f:\n",
        "    pickle.dump(X_not_filtered_texts, f)\n",
        "#with open ('data.pickle', 'rb') as f:\n",
        "   #X_not_filtered_texts = pickle.load(f)\n",
        "\n",
        "X_filtered_texts = []\n",
        "for text in X_not_filtered_texts:\n",
        "  words = []\n",
        "  for word in text:\n",
        "    if word in w2v.vocab:\n",
        "      words.append(word)\n",
        "  if len(words) == 0:\n",
        "    print (\"asd\")\n",
        "  X_filtered_texts.append(words)"
      ],
      "metadata": {
        "id": "DOzyNAoSmz5Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fvpCzQdjTU5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [list(map(lambda x: float(w2v.vocab[x].index), t)) for t in X_filtered_texts]\n",
        "X = pad_sequences(X, maxlen=100)\n",
        "X = np.asarray(X).astype('float32')\n",
        "\n",
        "Y = np.array(df_news['is_fake']).astype('float32').reshape((-1,1))\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y)"
      ],
      "metadata": {
        "id": "bf55rhNGm0qZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание модели для обработки текста"
      ],
      "metadata": {
        "id": "BTwlVVDdTdsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model = Sequential()\n",
        "\n",
        "weights = w2v.vectors\n",
        "layer = Embedding(\n",
        "    input_dim=weights.shape[0],\n",
        "    output_dim=weights.shape[1],\n",
        "    weights=[weights],\n",
        "    input_length=100,\n",
        "    mask_zero=True,\n",
        "    trainable=False,\n",
        ")\n",
        "\n",
        "seq_model.add(layer)\n",
        "seq_model.add(Dense(50, activation='relu'))\n",
        "seq_model.add(Flatten())\n",
        "seq_model.add(Dropout(0.6))\n",
        "seq_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "seq_model.compile(loss='mean_squared_logarithmic_error',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "seq_model.fit(x_train, y_train, epochs=20,\n",
        "               validation_data=(x_test, y_test))\n",
        "\n",
        "seq_model.summary()\n",
        "# seq_model.save(\"model.keras\")"
      ],
      "metadata": {
        "id": "hfZ6b96Sm4F6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29cd0a5b-bd22-4ea0-cfae-fde4f5d81894"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "135/135 [==============================] - 7s 42ms/step - loss: 0.1372 - accuracy: 0.6394 - val_loss: 0.0734 - val_accuracy: 0.7611\n",
            "Epoch 2/20\n",
            "135/135 [==============================] - 3s 19ms/step - loss: 0.0763 - accuracy: 0.7677 - val_loss: 0.0659 - val_accuracy: 0.7958\n",
            "Epoch 3/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0662 - accuracy: 0.8064 - val_loss: 0.0624 - val_accuracy: 0.8069\n",
            "Epoch 4/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0612 - accuracy: 0.8231 - val_loss: 0.0607 - val_accuracy: 0.8153\n",
            "Epoch 5/20\n",
            "135/135 [==============================] - 3s 21ms/step - loss: 0.0588 - accuracy: 0.8231 - val_loss: 0.0604 - val_accuracy: 0.8167\n",
            "Epoch 6/20\n",
            "135/135 [==============================] - 3s 19ms/step - loss: 0.0560 - accuracy: 0.8356 - val_loss: 0.0604 - val_accuracy: 0.8229\n",
            "Epoch 7/20\n",
            "135/135 [==============================] - 3s 20ms/step - loss: 0.0521 - accuracy: 0.8469 - val_loss: 0.0606 - val_accuracy: 0.8062\n",
            "Epoch 8/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0520 - accuracy: 0.8490 - val_loss: 0.0596 - val_accuracy: 0.8181\n",
            "Epoch 9/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0506 - accuracy: 0.8488 - val_loss: 0.0592 - val_accuracy: 0.8194\n",
            "Epoch 10/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0480 - accuracy: 0.8592 - val_loss: 0.0596 - val_accuracy: 0.8257\n",
            "Epoch 11/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0454 - accuracy: 0.8692 - val_loss: 0.0616 - val_accuracy: 0.8125\n",
            "Epoch 12/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0464 - accuracy: 0.8659 - val_loss: 0.0606 - val_accuracy: 0.8188\n",
            "Epoch 13/20\n",
            "135/135 [==============================] - 3s 19ms/step - loss: 0.0452 - accuracy: 0.8685 - val_loss: 0.0614 - val_accuracy: 0.8160\n",
            "Epoch 14/20\n",
            "135/135 [==============================] - 3s 19ms/step - loss: 0.0427 - accuracy: 0.8798 - val_loss: 0.0606 - val_accuracy: 0.8208\n",
            "Epoch 15/20\n",
            "135/135 [==============================] - 3s 19ms/step - loss: 0.0413 - accuracy: 0.8874 - val_loss: 0.0607 - val_accuracy: 0.8167\n",
            "Epoch 16/20\n",
            "135/135 [==============================] - 3s 19ms/step - loss: 0.0413 - accuracy: 0.8786 - val_loss: 0.0604 - val_accuracy: 0.8299\n",
            "Epoch 17/20\n",
            "135/135 [==============================] - 3s 19ms/step - loss: 0.0402 - accuracy: 0.8874 - val_loss: 0.0619 - val_accuracy: 0.8153\n",
            "Epoch 18/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0397 - accuracy: 0.8891 - val_loss: 0.0605 - val_accuracy: 0.8257\n",
            "Epoch 19/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0407 - accuracy: 0.8833 - val_loss: 0.0608 - val_accuracy: 0.8306\n",
            "Epoch 20/20\n",
            "135/135 [==============================] - 2s 18ms/step - loss: 0.0369 - accuracy: 0.8986 - val_loss: 0.0608 - val_accuracy: 0.8257\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 300)          74799900  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100, 50)           15050     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 5000)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5000)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 5001      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 74,819,951\n",
            "Trainable params: 20,051\n",
            "Non-trainable params: 74,799,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "seq_model = load_model(\"model.keras\")"
      ],
      "metadata": {
        "id": "To6zVReldX-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Набор новостей, на которых происходит тестирование (фейковые 2ая и 5ая)"
      ],
      "metadata": {
        "id": "Q9mgRwATTqkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_texts = ['Новак спрогнозировал новый мировой энергокризис через 5-10 лет',\n",
        "              'Во всех школах России установят счетчик американского госдолга',\n",
        "              'РФ продолжает рассматривать европу как потенциальный рынок для сбыта газа',\n",
        "              'Число погибших от последствий зимнего шторма в США выросло до 28',\n",
        "              'В магазинах появились дешевые яйца без желтков']"
      ],
      "metadata": {
        "id": "8Qk14NhygvnE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "И обработка созданного набора"
      ],
      "metadata": {
        "id": "n3f4RFauT9Hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_preproc = [udp_tagging(text_preprocessing(fake_text)) for fake_text in fake_texts]\n",
        "\n",
        "fake_words_tagged = []\n",
        "for text in fake_preproc:\n",
        "  words = []\n",
        "  for word in text:\n",
        "    if word in w2v.vocab:\n",
        "      words.append(word)\n",
        "  fake_words_tagged.append(words)\n",
        "\n",
        "fake_indexed = [list(map(lambda x: float(w2v.vocab[x].index), t)) for t in fake_words_tagged]\n",
        "fake_indexed = pad_sequences(fake_indexed, maxlen=100)\n",
        "fake_indexed = np.asarray(fake_indexed).astype('float32')\n",
        "\n",
        "print(seq_model.predict(fake_indexed))"
      ],
      "metadata": {
        "id": "nJY-KzWHm5Hz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a300e24-5237-4e3d-c254-bd2f46629a7a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 102ms/step\n",
            "[[0.14209108]\n",
            " [0.93135417]\n",
            " [0.08578382]\n",
            " [0.49240673]\n",
            " [0.4518472 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тестирование нейронной сети на тестовом наборе\n",
        "\n",
        "(как оказалось в файле с тестовым набором лежат и фейковые, и реальные новости, но помечены все меткой реальных новостей, поэтому это тестирование не является валидным)"
      ],
      "metadata": {
        "id": "FX0JBpljUD-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv ('test.tsv', sep='\\t')\n",
        "fake_preproc = test['title'].apply(lambda x: udp_tagging(text_preprocessing(x))).array\n",
        "fake_words_tagged = []\n",
        "for text in fake_preproc:\n",
        "  words = []\n",
        "  for word in text:\n",
        "    if word in w2v.vocab:\n",
        "      words.append(word)\n",
        "  fake_words_tagged.append(words)\n",
        "Y = np.array(test['is_fake']).astype('float32').reshape((-1,1))\n",
        "\n",
        "fake_indexed = [list(map(lambda x: float(w2v.vocab[x].index), t)) for t in fake_words_tagged]\n",
        "fake_indexed = pad_sequences(fake_indexed, maxlen=100)\n",
        "fake_indexed = np.asarray(fake_indexed).astype('float32')\n",
        "\n",
        "print(seq_model.evaluate(fake_indexed, Y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcxDpi0tHznu",
        "outputId": "4263c42f-128f-47d3-8cb3-767ead701b3a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 9ms/step - loss: 0.1960 - accuracy: 0.5610\n",
            "[0.19597852230072021, 0.5609999895095825]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Работающая нейронная сеть (вау)"
      ],
      "metadata": {
        "id": "MIe5ZMnwUYBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лемматизация текста и применение tfidf векторайзера. И скармливание это классификатору из Sklearn"
      ],
      "metadata": {
        "id": "pP8Pew9SUezu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "fake_preproc = df_news['title'].apply(lambda x: \" \".join(text_preprocessing(x))).array\n",
        "Y = np.array(df_news['is_fake']).astype('float32').reshape((-1,1))\n",
        "x_train, x_test, y_train, y_test = train_test_split(fake_preproc, Y)\n",
        "tfidf = TfidfVectorizer()\n",
        "vec_train = tfidf.fit_transform(x_train)\n",
        "vec_val = tfidf.transform(x_test)\n",
        "\n",
        "pac = PassiveAggressiveClassifier(C = 0.01)\n",
        "\n",
        "pac.fit(vec_train, y_train)\n",
        "val_pred = pac.predict(vec_val)\n",
        "print(classification_report(y_test, val_pred))\n",
        "\n",
        "# Тестировние на тестовом наборе данных\n",
        "fake_preproc = test['title'].apply(lambda x: \" \".join(text_preprocessing(x))).array\n",
        "Y = np.array(test['is_fake']).astype('float32').reshape((-1,1))\n",
        "vec_val = tfidf.transform(fake_preproc)\n",
        "val_pred = pac.predict(vec_val)\n",
        "print(classification_report(Y, val_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpYVJX4ONsn1",
        "outputId": "da02507f-feab-4607-b4da-a22cd3308a6b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.82      0.84       722\n",
            "         1.0       0.83      0.86      0.84       718\n",
            "\n",
            "    accuracy                           0.84      1440\n",
            "   macro avg       0.84      0.84      0.84      1440\n",
            "weighted avg       0.84      0.84      0.84      1440\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.48      0.65      1000\n",
            "         1.0       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.48      1000\n",
            "   macro avg       0.50      0.24      0.32      1000\n",
            "weighted avg       1.00      0.48      0.65      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тестирование на созданном наборе новостей"
      ],
      "metadata": {
        "id": "9tLsceq7VCQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_preproc = [\" \".join(text_preprocessing(fake_text)) for fake_text in fake_texts]\n",
        "vec_val = tfidf.transform(fake_preproc)\n",
        "val_pred = pac.decision_function(vec_val)\n",
        "val_pred0 = pac.predict(vec_val)\n",
        "print(val_pred, val_pred0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hHCmLFDfLFb",
        "outputId": "dab42c21-fab8-46d8-faa3-4472285a99d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.40157504  1.10566439 -0.69489376 -0.13854861  0.23341085] [0. 1. 0. 0. 1.]\n"
          ]
        }
      ]
    }
  ]
}